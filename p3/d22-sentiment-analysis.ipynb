{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment analysis on grab SPAC\n",
    "\n",
    "## Carried forward from d20. search \"d22 continue here\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Obtain"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\n",
    "\n",
    "# grab SPAC\n",
    "url = 'https://raw.githubusercontent.com/dwihdyn/ds-exploration/main/p3/data/grab-spac.txt'\n",
    "\n",
    "# # 'great gatsby' book from gutenberg.org\n",
    "# url = 'https://www.gutenberg.org/files/64317/64317-0.txt'\n",
    "\n",
    "# load .txt to our notebook\n",
    "response = requests.get(url)\n",
    "ori_text = response.text\n",
    "\n",
    "# print(ori_text)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scrub\n",
    "\n",
    "- remove punctuations & set all to lowecase\n",
    "- tokenize words\n",
    "- remove stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# list of punctuations u want to remove\n",
    "text = ori_text\n",
    "punc = '''\\n!()-[]{};:'\"\\, <>./?@#$%^&*_~\\rÃ¢\\x80\\x9d\\x9c\\x99\\x94'''\n",
    "\n",
    "# remove punctuation by looping thru each character in the text, and replace punc with space ' '\n",
    "for char in text:\n",
    "    if char in punc:\n",
    "        text = text.replace(char, ' ')\n",
    "\n",
    "# set all to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# print(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# tokenize word (split whole sentence to each letter) by space ' '\n",
    "\n",
    "tokens = text.split(' ')\n",
    "# print(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# bring in pre-made list of stopwords\n",
    "sw_url = 'https://raw.githubusercontent.com/dwihdyn/ds-exploration/main/p3/stopwords-nlp.txt'\n",
    "response = requests.get(sw_url)\n",
    "stopwords = response.text.splitlines()\n",
    "\n",
    "# remove stopwords using loop\n",
    "tokens = [t for t in tokens if t not in stopwords and len(t) > 0]\n",
    "\n",
    "# print(token)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explore\n",
    "\n",
    "- get top50 most common letter in the article\n",
    "- sentiment analysis on each sentence in the paragraph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from collections import Counter\n",
    "\n",
    "# get top50 most common letter in the article\n",
    "print(Counter(tokens).most_common(50))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 29), ('and', 23), ('in', 23), ('grab', 22), ('a', 21), ('of', 15), ('s', 15), ('to', 14), ('with', 11), ('billion', 11), ('on', 9), ('is', 9), ('for', 8), ('as', 7), ('year', 7), ('will', 7), ('business', 7), ('its', 7), ('it', 7), ('altimeter', 6), ('growth', 6), ('by', 6), ('food', 5), ('delivery', 5), ('southeast', 5), ('u', 5), ('said', 5), ('was', 5), ('last', 5), ('asia', 4), ('company', 4), ('listing', 4), ('deal', 4), ('are', 4), ('at', 4), ('tan', 4), ('reuters', 4), ('digital', 4), ('which', 4), ('singapore', 3), ('merger', 3), ('after', 3), ('investors', 3), ('such', 3), ('4', 3), ('investment', 3), ('financial', 3), ('transactions', 3), ('up', 3), ('services', 3)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d22 continue here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# separate long paragraph into each sentences (total 28)\n",
    "sentences = list(filter(lambda x : x != '', ori_text.split('\\n\\n')))\n",
    "\n",
    "print(sentences[10])\n",
    "print(sentences[23])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reuters earlier reported that Grab would announce the deal on Tuesday.\n",
      "The listing will give Grab extra firepower in its main market of Indonesia, where local rival Gojek is close to sealing a merger with the country's leading e-commerce business, Tokopedia.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# load packages\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get sentiment score of each sentences using NLTK\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_score = []\n",
    "\n",
    "for i in sentences:\n",
    "    final_score = analyzer.polarity_scores(i)\n",
    "    sentiment_score.append(final_score.get('compound'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sentiment_score"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5994,\n",
       " 0.128,\n",
       " 0.5106,\n",
       " 0.0,\n",
       " 0.3612,\n",
       " 0.5106,\n",
       " 0.6124,\n",
       " 0.0,\n",
       " 0.128,\n",
       " 0.7717,\n",
       " 0.0,\n",
       " 0.6705,\n",
       " 0.5423,\n",
       " 0.0,\n",
       " 0.6597,\n",
       " 0.7003,\n",
       " -0.0516,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0258,\n",
       " -0.4019,\n",
       " 0.5256,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5719,\n",
       " 0.4404,\n",
       " 0.802,\n",
       " 0.0]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(sentences[16])\n",
    "print(sentiment_score[16])\n",
    "print('-----')\n",
    "print(sentences[20])\n",
    "print(sentiment_score[20])\n",
    "print('-----')\n",
    "print('Battle is a negative word, dont fight guys')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Grab attracted global attention in 2018 when it acquired Uber's Southeast Asia business after a costly five-year battle in return for a stake in itself.\n",
      "-0.0516\n",
      "-----\n",
      "BATTLEGROUND INDONESIA\n",
      "-0.4019\n",
      "-----\n",
      "Battle is a negative word, dont fight guys\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# iNterpret"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('quantra38': conda)"
  },
  "interpreter": {
   "hash": "ba02ef682c4040d20aeff24505da2d61e29df5e4d95d58036fbb82853e11aae6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}